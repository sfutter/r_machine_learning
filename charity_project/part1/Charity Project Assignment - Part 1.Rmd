---
title: "Charity Project - Part 1"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
### 1. Read data from csv file
a. Here I use the na.strings="NA" argument to encode the missing values. 
b. I set stringsAsFactors=TRUE 
c. Finally, I use the colClasses argument to update DONR, HOME, and HINC from integers to factors. Note that the additional colClasses option was added because DONR, HOME, and HINC were not automatically set to factors. For the purpose of this analysis it is necessary for these variables to be factors.
```{r}
setwd('/Users/stevenfutter/Dropbox/NU/MACHINE_LEARNING/charity_project/part1/')
charity = read.csv('projectDataPart1.csv', header=T, na.strings="NA",
                   stringsAsFactors = TRUE,
                   colClasses=c("DONR"="factor", "HOME"="factor","HINC"="factor"))

# set up $ expansion
attach(charity)

# check that column types are set up correctly
str(charity)

# confirm size of data table
dim(charity) # 3684 21

# take a peek at the data
head(charity)
```


### 2. Data Quality Check
a. We now look at a quick summary of the values of the data to get a feel for the value ranges, shape of the distributions, and the number of missing values for each variable in the dataset. To do this we use R's Hmisc library to describe the dataset, as below. 

As can be seen in the output MEDHVAL is highly skewed since the mean and median numbers are quite different. Other variables have extreme high and low values. For example, RAMNTALL, the dollar amount of lifetime gifts to date, has 290.8 as the 95% percentile value, but the highest values are 1190, 1610, 1622, 1765, and 2200. Other variables are skewed, but this is better highlighted by the matrix of histograms. See part (c) below. 

```{r, message=FALSE}
library(Hmisc)
Hmisc::describe(charity)
```

b. As can be seen in the above output, Household Income (HINC) has 453 missing values. Depending on the analysis we choose we may need to impute some of these missing values before fitting any models. By including missing values into a training model, the model is likely to produce less accurate predictive accuracy.

c. As mentioned in part (a), besides the missing values it appears that many of the variables in the data set are either positively or negatively skewed. This is best represented by a matrix of histograms, as below. The skewness or non-normality in some of the variables of the dataset may cause issues for some type of model fits which rely upon normality in the residuals. In addition, many of the variables have extreme values: RAMNTALL, MAXRAMNT, and LASTGIFT appear to have especially large extreme values. 

```{r, message=FALSE, fig.width=12, fig.height=12}
library(plyr)
library(psych)
multi.hist(charity[,sapply(charity, is.numeric)])
```


### 3. Exploratory Data Analysis (EDA)
We now carry out some exploratory data analysis to look for interesting relationships in the data. 

a. First, we use R to perform EDA for the dataset provided. The response for the regression problem is DAMT. Note that ID is for identification purposes only and is not to be used as a predictor, so we remove it here:

```{r}
df = charity[,-1]   # remove ID variable
names(df)
```

b. Now that ID has been removed from the charity data frame we report findings from the EDA. By plotting the correlation between each variable and DAMT, we find that the most correlated variables are: LASTGIFT (0.72), MAXRAMNT (0.41), and RAMNTALL (0.24). We run the correlation between all variables and DAMT from the R code below:

```{r}
# create new data.frame (df.num) with numeric and integer values only to plot correlations
df.num = df[,sapply(df, class) %in% c("numeric", "integer") ]

# calculate the correlations between each variable and DAMT
apply(df.num,2, function(col)cor(col, df.num$DAMT))

# build scatterplots between numeric variables
par(mfrow=c(1,3))
plot(df.num$LASTGIFT, df.num$DAMT)
plot(df.num$MAXRAMNT, df.num$DAMT)
plot(df.num$RAMNTALL, df.num$DAMT)
```

Let's also include boxplots of the categorical variables to see which of those may be predictive. From the boxplots of the categorical variables against DAMT we can see that there are some extreme values for each variable. However, median values for HOME and HINC are quite similar, whereas median donation values for RFA_96 and RFA_97 vary across each of the factors showing some potential to be good predictors. There will be an opportunity to break out the RFA statuses in the next section. The R code below builds the boxplots for the categorical variables HOMC, HINC, RFA_96, and RFA_97. 

```{r}
par(mfrow=c(2,2))
boxplot(DAMT~HOME, data=charity, col='lightgray',main="Charity Data - HOME", 
        xlab="Home Owner (HOME)", 
        ylab="Donation Amount (in $)")

boxplot(DAMT~HINC, data=charity, col='lightgray',main="Charity Data - HINC", 
        xlab="Household Income (HINC)", 
        ylab="Donation Amount (in $)")

boxplot(DAMT~RFA_97, data=charity, col='lightgray',main="Charity Data - RFA_97",
        xlab="Donor's RFA status as of 1997 Promo date (RFA_97)", 
        ylab="Donation Amount (in $)")

boxplot(DAMT~RFA_96, data=charity, col='lightgray',main="Charity Data - RFA_96",
        xlab="Donor's RFA status as of 1996 Promo date (RFA_96)", 
        ylab="Donation Amount (in $)")
```


### 4. Data Preparation

a. Handling of Missing Values:
The missing values are found in the household income (HINC) variable. Given that the box plots showed very similar levels of donations across each of the seven incomes levels, I decided to remove the HINC variable from the analysis. 

```{r}
df1 = charity[,-6]  #remove HINC
names(df1)
```

b. Treatment of transformed variables added to the dataset:
Although many of the variables have extreme values, I decided to leave them in place. Although the values appear to be larger than others it also appears that they may be valid. We instead tranform the three variables most correlated with DAMT to reign in the outliers to see if the logs of each variable improves the model fit. 

Log values are taken for: MAXRAMNT and RAMNTALL, as below.  
By taking the logs we reign in the distribution of values across the x-axis, as can be seen from the output below. 

```{r}
par(mfrow=c(2,2))
df1$LOG_MAXRAMNT = log(df1$MAXRAMNT)
plot(df1$MAXRAMNT,df1$DAMT, main='MAXRAMNT')
plot(df1$LOG_MAXRAMNT,df1$DAMT, main='Log(MAXRAMNT)')

df1$LOG_RAMNTALL = log(df1$RAMNTALL)
plot(df1$RAMNTALL,df1$DAMT, main='RAMNTALL')
plot(df1$LOG_RAMNTALL,df1$DAMT, main='Log(RAMNTALL)')

```

c. Re-categorization of categorical variables:
There is a lot of information compiled into the three character code variables: RFA_96 and RFA_97. These two variables covering a donor's RFA status as of the 1996/1997 promotion date include a character for RECENCY, FREQUENCY, and AMOUNT. We have broken these two variables into five variables. Note that RECENCY_97 is not included as an additional variable because all donors in this column are 'L', i.e. lapsing donors. 

```{r}
#df1$RECENCY_97   = substr(RFA_97,1,1)   #removed as only 1 factor found.
df1$FREQUENCY_97 = substr(RFA_97,2,2)
df1$AMOUNT_97    = substr(RFA_97,3,3)

df1$RECENCY_96   = substr(RFA_96,1,1)
df1$FREQUENCY_96 = substr(RFA_96,2,2)
df1$AMOUNT_96    = substr(RFA_96,3,3)

names(df1)
```

```{r}
str(df1)
```

d. Variables that have been removed from the dataset:
To this point, HINC has already been removed from the dataset due to its high number of missing values. Additionally, we remove RFA_96 and RFA_97 as we created five new variables above with fewer factors in each variable that replace the need for RFA_96 and RFA_97. Per the notes we also remove the ID and DONR variables from the analysis.

```{r}
df1 = df1[,c(-1,-2,-19,-20)]  # remove ID, DONR, RFA_96, and RFA_97 in addition to HINC removed previously
names(df1)
```



### 5. Dataset Partitioning
In this next section we employ a hold-out test dataset for model validation and selection.

a. Creation of Hold-Out Test Set & b. Training Set
In the R code below we take a sample of 25% of the observations in the dataset to form a hold-out test set. This data will be referred to as the test set for the remainder of the document. The remaining 75% form the training set. See R code below for creation of ch.train and ch.test. 

```{r}
dim(df1) # 3684 x 23
names(df1)

smp.size = floor(0.75 * nrow(df1))

set.seed(1)
train = sample(seq_len(nrow(df1)), size = smp.size)
test = -train

ch.train = df1[train,]
ch.test = df1[-train,]

dim(ch.train)
summary(ch.train)

dim(ch.test)
summary(ch.test)
```

From the above summary() output we can see that the training set consists of 2763 observations (i.e. 75% of 3684) and 23 variables, and the test set consists of 921 observations (i.e. 25% of 3684) and 23 variables.The DAMT response variable has a range of 1 to 200 in both the training and test data sets. The median for the test set is 12.5, which is slightly lower than the median for the training set which is 15. 



### 6. Model Fitting
In this next section we use R to develop various models for the response variable DAMT. Note that per the instruction we have already removed the variables ID and DONR from the predictor set. We begin by fitting each model to the training set data only. 

#### a. Simple Linear Regression Model
####    MODEL 1:  DAMT = B0 + B1*LASTGIFT + e

For the first model we run a simple linear regression model using LASTGIFT as the single predictor, since this was the variable that was most correlated to DAMT with an r-value = 0.72. As can be seen from the output below the LASTGIFT variable is highly significant. It has an R^2 value of 0.56 which is quite high given that we are looking at a one-variable model.

```{r}
fit1 = lm(DAMT~LASTGIFT,ch.train)
summary(fit1)
plot(fit1)
```



Looking at the plot of residuals against the fitted values from the fit1 model it looks as though the distribution is homoskedastic and therefore the variance in error terms is reasonably constant. In addition, the QQ-plot follows the line and appears to be approximately normal. Therefore, we can trust the output of the simple linear regression model.


#### b. Multiple Linear Regression Model with Best Subset Selection
Before we begin this next model we need to first convert the newly added columns to factors. We do this below:

```{r}
df1$FREQUENCY_97 = factor(df1$FREQUENCY_97)
df1$AMOUNT_97    = factor(df1$AMOUNT_97)
df1$RECENCY_96   = factor(df1$RECENCY_96)
df1$FREQUENCY_96 = factor(df1$FREQUENCY_96)
df1$AMOUNT_96    = factor(df1$AMOUNT_96)
str(df1)
```

####    MODEL 2: B0 + B1*HOME + B2*GENDER + B3*RAMNTALL + B4*NGIFTALL + B5*LASTGIFT + B6*LOG_MAXRAMNT + B7*AMOUNT_97F + B8*AMOUNT_97G + B9*FREQUENCY_96 + e

The multiple linear regression model above was created by using the best subset selection algorithm. The regsubsets() function performs best subset selection by identifying the best model that contains a given number of predictors, where 'best' is quantified using RSS. The asterixes in the output below indicate that the variable is included in the model 2, above.

```{r}
library(leaps)
fit2=regsubsets(DAMT~., data=ch.train)
summary(fit2)
```


Note that we do not use the nvmax option in the regsubsets() function. I made this decision for ease of interpretability of the final model. A model with fewer variables is more easily interpreted. Let's now move on to shrinkage models.

#### c. Shinkage Models 
####    MODEL 3: Lasso Model:    DAMT = B0 - B1*NGIFTALL + B2*LASTGIFT + B3*LOG_MAXRAMNT + B4*AMOUNT_97(G) + B5*FREQUENCY_96(1) + e

```{r}
library(glmnet)
x = model.matrix(DAMT~., df1)
y=df1$DAMT

grid=10^seq(10,-2,length=100) 
fit3 = glmnet(x[train,], y[train], alpha=1, lambda=grid)  # alpha=1 is the LASSO model
```

Instead of arbitrarily choosing a tuning parameter, lambda, we use cross-validation to choose a tuning parameter lambda.
With the lasso model it is important to set the appropriate value of lambda, as we will see with the plot below. 

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min                          
bestlam                       # 0.6740771 lambda is the best lambda value. We will use this later on to calculate the training and test data MSE's.
```

We report the results of the lasso model as below. Notice that only five of the variables are included in the lasso model, as written out above at the beginning of section (c). We notice that NGIFTALL is has a negative effect on DAMT. As the number of lifetime gifts increase by one unit the expected DAMT decreases by $0.02376012.

```{r}
lasso.coef=predict(fit3,type='coefficients',s=bestlam)
lasso.coef                       # 3 out of 10 coef's are non-zero
lasso.coef[lasso.coef!=0]
```

As can be seen the lasso with lambda chosen by cross-validation has only 5 variables making it easier to interpret than other models. Let's see how it performs on test data before we help the charity make any decisions on their next promotion. 


#### 4. Decision-Tree Model
####    MODEL 4

```{r}
# install.packages('tree')
# The tree library is used to construct classification and regression trees.
library(tree)

fit4=tree(DAMT~.,df1,subset=train) # fitting regression tree to training set.
summary(fit4)                            
par(mfrow=c(1,1))
plot(fit4)
text(fit4, pretty=0)
```


In the decision-tree model we see that LASTGIFT is the most predictive variable as this is the first branch in the tree. Last gift values less that <21.5 are not expected to exceed a DAMT value of 19.29. Last gift values greater than 21.5 may be expected to be higher than 27.68. Other inputs used in the decision-tree which can be thought of as being the most predictive inputs to the model are MAXRAMNT and AMOUNT_97. All other variables are not included in the decision-tree which is good information for the charity. Let's first see how each model performs on the test data set by measuring the MSE's for the four models created above.



### 7. Model Validation
(a) - (c) In this next section we evaluate the performance of each of the four models created above. First we much calculate the test and training MSE's for each model. 

#### Model 1 : Calculate the MSE of train and test data
```{r}
mean((DAMT-predict(fit1, df1))[train]^2)      # MSE Training Set = 65.18256
mean((DAMT-predict(fit1, df1))[test]^2)       # MSE Test Set     = 79.24383
fit1.train.mse = mean((DAMT-predict(fit1, df1))[train]^2)  
fit1.test.mse  = mean((DAMT-predict(fit1, df1))[test]^2)  
```

#### Model 2 : Calculate the MSE of train and test data
We repeat the fit using lm() so that we can use the predict() function for calculating MSE.
```{r}
fit2=lm(DAMT~HOME+GENDER+RAMNTALL+NGIFTALL+LASTGIFT+LOG_MAXRAMNT+AMOUNT_97+AMOUNT_97+FREQUENCY_96, data=ch.train)
mean((DAMT-predict(fit2, df1))[train]^2)      # MSE Training Set = 59.37364
mean((DAMT-predict(fit2, df1))[test]^2)       # MSE Test Set     = 73.79594
fit2.train.mse = mean((DAMT-predict(fit2, df1))[train]^2)  
fit2.test.mse  = mean((DAMT-predict(fit2, df1))[test]^2) 
```

#### Model 3 : Calculate the MSE of train and test data
As there is no predict() method for regsubsets(), we create our own predict() method, as below:
```{r}
  predict.regsubsets=function(object, newdata, id, ...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object, id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

y.test=y[test]
y.train=y[train]
lasso.pred=predict(fit3,s=bestlam,newx=x[train,])
mean((lasso.pred-y.train)^2)                        # 61.35197 is train MSE
fit3.train.mse = mean((lasso.pred-y.train)^2) 

lasso.pred=predict(fit3,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)                         # 74.24213 is the MSE
fit3.test.mse  = mean((lasso.pred-y.test)^2) 
  
```


<!-- predict(fit3,s=bestlam,newx=x[test,]) -->

#### Model 4 MSE : Calculate the MSE of train and test data
```{r}
yhat=predict(fit4,newdata=df1[train,])
mean((yhat-y.train)^2)          # 63.87194
fit4.train.mse = mean((yhat-y.train)^2)

yhat=predict(fit4,newdata=df1[test,])
mean((yhat-y.test)^2)          # 75.07405
fit4.test.mse  = mean((yhat-y.test)^2) 
```


a continued. Build a table (in your document) that has one row for each model you fit in Exercise 6. The table should have three columns (at minimum): Model Name, Training Set MSE, and Test Set MSE. You can include additional columns if you would like.
```{r}
Model     = c('Model 1','Model 2', 'Model 3', 'Model 4')
Train_MSE = c(round(fit1.train.mse,3), round(fit2.train.mse,3), round(fit3.train.mse,3), round(fit4.train.mse,3))
Test_MSE  = c(round(fit1.test.mse,3), round(fit2.test.mse,3), round(fit3.test.mse,3), round(fit4.test.mse,3))
table = cbind(Model, Train_MSE, Test_MSE)
table
```

#### 8. Model Selection
Using the data presented in the table above we make a decision as to which model we will carry forward to the next charity project exercise. 
a. As expected, the predictive accuracy of each model varies. The Test MSE for Model 2 of 73.796 is the lowest which means that this model is most likely to perform the best when it comes to predictive accuracy. Model 2 is the multiple linear regression model with nine explanatory variables. Not surprisingly, the worst performing model was model 1 with a test MSE of 79.244. Measuring DAMT with only one predictor variable, LASTGIFT insufficiently explains the variation in the response, DAMT. 

b. Although the best performing model based upon the test MSE scores was the multiple linear regression model, I have decided to carry forward model 3, the lasso model. This model has only five variables and a similar test MSE of 74.242. Having almost half the number of variables as the multiple linear regression model makes this model particularly easy to comprehend. Additionally, the difference between the test and training MSE in the Lasso model is smaller than the difference in the training and test MSE of the multiple linear regression model. This leads me to believe that the multiple linear regression model may have been slightly overfit to the training data and may not lead to as good of a performance over continuously repeated test samples. 